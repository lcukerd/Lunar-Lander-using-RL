{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,state_size,action_size,epsilon,epsilon_decay):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = 100\n",
    "        self.learning_rate = 0.001\n",
    "        self.Epsilon = epsilon\n",
    "        self.Gamma = 0.7\n",
    "        self.Epsilon_decay = epsilon_decay\n",
    "        self.Epsilon_min = 0.001\n",
    "        self.memory = deque(maxlen = 20000)\n",
    "        self.model = self.buildModel()\n",
    "    \n",
    "    def buildModel(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12,input_dim = self.state_size,activation = 'relu'))\n",
    "        model.add(Dense(6,activation = 'relu'))\n",
    "        model.add(Dense(self.action_size,activation = 'softmax'))\n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def chooseAction(self,state):\n",
    "        if (np.random.uniform() <= self.Epsilon):\n",
    "            return random.randrange(self.action_size)\n",
    "        action = self.model.predict(state)\n",
    "        return np.argmax(action)\n",
    "    \n",
    "    def store(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "    def replay(self):\n",
    "        if (len(self.memory)<self.batch_size):\n",
    "            batch = random.sample(self.memory,len(self.memory))\n",
    "        else:\n",
    "            batch = random.sample(self.memory,self.batch_size)\n",
    "        cost = 0\n",
    "        for state,action,reward,next_state,done in batch:\n",
    "            if done:\n",
    "                target  = reward\n",
    "            else:\n",
    "                target = reward + self.Gamma * np.amax(self.model.predict(next_state))\n",
    "            current = self.model.predict(state)\n",
    "            cost += abs(target - current[0][action])\n",
    "            current[0][action] = target\n",
    "            self.model.fit(state,current,epochs=1,verbose=0)\n",
    "        if (self.Epsilon > self.Epsilon_min):\n",
    "            self.Epsilon *= self.Epsilon_decay\n",
    "        \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "Episodes = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "8 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "print (state_size,action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size,action_size,1.0,0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 832/5000, e: 0.015, Time: 638, Reward: 141.56798235600345,Steps: 499:::\n",
      "Episode: 843/5000, e: 0.015, Time: 174, Reward: 8.564117948573767,Steps: 134::::\n",
      "Episode: 846/5000, e: 0.014, Time: 514, Reward: 132.11171099348587,Steps: 4254::\n",
      "Episode: 850/5000, e: 0.014, Time: 999, Reward: 27.911520198310654,Steps: 8993::\n",
      "Episode: 855/5000, e: 0.014, Time: 999, Reward: 20.086323228162996,Steps: 713:::\n",
      "Episode: 872/5000, e: 0.013, Time: 899, Reward: 77.80860034159966,Steps: 805::::\n",
      "Episode: 873/5000, e: 0.013, Time: 738, Reward: 129.0812065669326,Steps: 642\n",
      "Episode: 874/5000, e: 0.012, Time: 450, Reward: 134.99193802678892,Steps: 407\n",
      "Episode: 877/5000, e: 0.012, Time: 652, Reward: 102.66776384519538,Steps: 5543::\n",
      "Episode: 878/5000, e: 0.012, Time: 875, Reward: 157.63716595671647,Steps: 709\n",
      "Episode: 879/5000, e: 0.012, Time: 449, Reward: 158.1197482856443,Steps: 394\n",
      "Episode: 882/5000, e: 0.012, Time: 511, Reward: 134.3029345082059,Steps: 44051::\n",
      "Episode: 883/5000, e: 0.012, Time: 999, Reward: 82.83619598356869,Steps: 704\n",
      "Episode: 887/5000, e: 0.012, Time: 745, Reward: 102.08293934604725,Steps: 6624::\n",
      "Episode: 888/5000, e: 0.012, Time: 721, Reward: 80.31722268298559,Steps: 655\n",
      "Episode: 891/5000, e: 0.011, Time: 738, Reward: 91.88054484708027,Steps: 65846::\n",
      "Episode: 892/5000, e: 0.011, Time: 542, Reward: 87.76353222017016,Steps: 476\n",
      "Episode: 893/5000, e: 0.011, Time: 542, Reward: 137.50900692752913,Steps: 467\n",
      "Episode: 898/5000, e: 0.011, Time: 688, Reward: 78.71970330102295,Steps: 56255::\n",
      "Episode: 899/5000, e: 0.011, Time: 659, Reward: 50.48080658881932,Steps: 568\n",
      "Episode: 900/5000, e: 0.011, Time: 552, Reward: 32.389902091113626,Steps: 473\n",
      "Episode: 902/5000, e: 0.011, Time: 823, Reward: 57.84675903109199,Steps: 75042::\n",
      "Episode: 903/5000, e: 0.011, Time: 641, Reward: 83.21251555914324,Steps: 547\n",
      "Episode: 908/5000, e: 0.01, Time: 584, Reward: 156.33215677703575,Steps: 4823:::\n",
      "Episode: 914/5000, e: 0.01, Time: 598, Reward: 108.66575783574807,Steps: 544::\n",
      "Episode: 924/5000, e: 0.0097, Time: 224, Reward: 132.20944179660756,Steps: 1929::\n",
      "Episode: 1230/5000, e: 0.0021, Time: 495, Reward: 153.48166834352836,Steps: 4188::\n",
      "Episode: 1244/5000, e: 0.0019, Time: 654, Reward: 174.66137121883827,Steps: 539:::\n",
      "Episode: 1249/5000, e: 0.0019, Time: 559, Reward: 113.18533802351556,Steps: 4673::\n",
      "Episode: 1250/5000, e: 0.0019, Time: 692, Reward: 102.05159216579354,Steps: 585\n",
      "Episode: 1292/5000, e: 0.0015, Time: 616, Reward: 77.58318931582681,Steps: 5964:::\n",
      "Episode: 1407/5000, e: 0.001, Time: 284, Reward: 171.0290702636567,Steps: 224:::::\n",
      "Episode: 1421/5000, e: 0.001, Time: 162, Reward: 33.623107844439204,Steps: 131:::\n",
      "Episode: 1450/5000, e: 0.001, Time: 388, Reward: 137.90484789799535,Steps: 2850::\n",
      "Episode: 2051/5000, e: 0.001, Time: 326, Reward: 223.550570138519,Steps: 327:::::\n",
      "Episode: 2463/5000, e: 0.001, Time: 72, Reward: 19.05543131344544,Steps: 7368::::\n",
      "Episode: 2846/5000, e: 0.001, Time: 89, Reward: 17.810791349113316,Steps: 82:::::\n",
      "Episode: 3541/5000, e: 0.001, Time: 112, Reward: 4.952808553083713,Steps: 90:::::\n",
      "Episode: 3579/5000, e: 0.001, Time: 999, Reward: 19.971559872594348,Steps: 6407:::\n",
      "Episode: 3591/5000, e: 0.001, Time: 364, Reward: 138.53334967886508,Steps: 307::::\n",
      "Episode: 3595/5000, e: 0.001, Time: 906, Reward: 94.73100179203108,Steps: 753::::\n",
      "Episode: 3605/5000, e: 0.001, Time: 999, Reward: 52.45046778063568,Steps: 6351:::\n",
      "Episode: 3607/5000, e: 0.001, Time: 412, Reward: 197.9496767178489,Steps: 308::\n",
      "Episode: 3608/5000, e: 0.001, Time: 999, Reward: 73.56913083597408,Steps: 553\n",
      "Episode: 3610/5000, e: 0.001, Time: 350, Reward: 183.74741005116138,Steps: 289::\n",
      "Episode: 3611/5000, e: 0.001, Time: 256, Reward: 5.060755746898181,Steps: 185\n",
      "Episode: 3616/5000, e: 0.001, Time: 584, Reward: 141.17214807358425,Steps: 375:::\n",
      "Episode: 3620/5000, e: 0.001, Time: 427, Reward: 191.8215336873181,Steps: 26927::\n",
      "Episode: 3623/5000, e: 0.001, Time: 351, Reward: 192.10736873291,Steps: 309289::\n",
      "Episode: 3627/5000, e: 0.001, Time: 240, Reward: 4.270636406778166,Steps: 1833:::\n",
      "Episode: 3628/5000, e: 0.001, Time: 504, Reward: 190.65050374234076,Steps: 370\n",
      "Episode: 3633/5000, e: 0.001, Time: 815, Reward: 100.04303482246551,Steps: 5518::\n",
      "Episode: 3637/5000, e: 0.001, Time: 557, Reward: 186.5573710412791,Steps: 40852::\n",
      "Episode: 3642/5000, e: 0.001, Time: 699, Reward: 146.50634333944993,Steps: 4877::\n",
      "Episode: 3643/5000, e: 0.001, Time: 613, Reward: 165.8851121427441,Steps: 442\n",
      "Episode: 3654/5000, e: 0.001, Time: 408, Reward: 222.773632311231,Steps: 33438:::\n",
      "Episode: 3655/5000, e: 0.001, Time: 152, Reward: 5.09583856170147,Steps: 113\n",
      "Episode: 3656/5000, e: 0.001, Time: 412, Reward: 195.13193319800462,Steps: 293\n",
      "Episode: 3657/5000, e: 0.001, Time: 179, Reward: 21.129846819438967,Steps: 136\n",
      "Episode: 3665/5000, e: 0.001, Time: 236, Reward: 190.94949242746787,Steps: 2000::\n",
      "Episode: 3667/5000, e: 0.001, Time: 336, Reward: 138.8210453973987,Steps: 28634::\n",
      "Episode: 3690/5000, e: 0.001, Time: 309, Reward: 186.0910879001615,Steps: 26359::\n",
      "Episode: 3701/5000, e: 0.001, Time: 373, Reward: 109.33715213935842,Steps: 303:::\n",
      "Episode: 3714/5000, e: 0.001, Time: 967, Reward: 141.46119189463369,Steps: 618:::\n",
      "Episode: 3878/5000, e: 0.001, Time: 130, Reward: 9.592455347392658,Steps: 110::::\n",
      "Episode: 4165/5000, e: 0.001, Time: 344, Reward: 122.59765137963237,Steps: 278:::\n",
      "Episode: 4531/5000, e: 0.001, Time: 251, Reward: 91.6560891879555,Steps: 1966::::\n",
      "Episode: 4999/5000, e: 0.001, Time: 154, Reward: -135.6628321961469,Steps: 124:::\r"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "for e in range(Episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1,state_size])\n",
    "    steps = 0\n",
    "    total_reward = 0\n",
    "    for time in range(1000):\n",
    "        env.render()\n",
    "        action = agent.chooseAction(state)\n",
    "        if action != 1:\n",
    "            steps +=1\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        total_reward += reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.store(state,action,total_reward,next_state,done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    agent.replay()\n",
    "    if total_reward >= 0:\n",
    "        print(\"Episode: {}/{}, e: {:.2}, Time: {}, Reward: {},Steps: {}\"\n",
    "          .format(e, Episodes, agent.Epsilon,time,total_reward,steps),end = '\\n')            \n",
    "    else:\n",
    "        print(\"Episode: {}/{}, e: {:.2}, Time: {}, Reward: {},Steps: {}::\"\n",
    "          .format(e, Episodes, agent.Epsilon,time,total_reward,steps),end = '\\r')                 \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(\"Lunar_lander-dqn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
